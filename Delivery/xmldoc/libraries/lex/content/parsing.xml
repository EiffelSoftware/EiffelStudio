<document output="studio" title="Parsing: The Parse Library">
	<meta_data/>
	<paragraph>
		<heading>
			<size>2</size>OVERVIEW
		</heading>
		<paragraph>Parsing is the task of analyzing the structure of documents such as programs, specifications or other structured texts.</paragraph>
		<paragraph>Many systems need to parse documents. The best-known examples are compilers, interpreters and other software development tools; but as soon as a system provides its users with a command language, or processes input data with a non-trivial structure, it will need parsing facilities.</paragraph>
		<paragraph>This chapter describes the Parse library, which you can use to process documents of many different types. It provides a simple and flexible parsing scheme, resulting from the full application of object-oriented principles.</paragraph>
		<paragraph>Because it concentrates on the higher-level structure, the Parse library requires auxiliary mechanisms for identifying a document?s lexical components: words, numbers and other such elementary units. To address this need it is recommended, although not required, to complement Parse with the Lex library studied in the previous chapter.</paragraph>
		<paragraph>Figure 1 shows the inheritance structure of the classes discussed in this chapter.</paragraph>
		<image>
			<url>figure2.png</url>
		</image>
		<paragraph>Figure 1: Parse class structure</paragraph>
		<heading>
			<size>2</size>WHY USE THE PARSE LIBRARY
		</heading>
		<paragraph>Let us fist look at the circumstances under which you may want - or not want - to use the Parse library.</paragraph>
		<heading>
			<size>3</size>The Parse library vs. parser generators
		</heading>
		<paragraph>Parsing is a heavily researched area of computing science and many tools are available to generate parsers. In particular, the popular Yacc tool, originally developed for Unix, is widely used to produce parsers.</paragraph>
		<paragraph>In some cases Yacc or similar tools are perfectly adequate. It is also sometimes desirable to write a special-purpose parser for a language, not relying on any parser generator. Several circumstances may, however, make the Parse library attractive:</paragraph>
		<list ordered="false">
			<item>The need to interface the parsing tasks with the rest of an object-oriented system (such as a compiler or more generally a "document processor" as defined below) in the simplest and most convenient way.</item>
			<item>The desire to apply object-oriented principles as fully as possible to all aspects of a system, including parsing, so as to gain the method?s many benefits, in particular reliability, reusability and extendibility.</item>
			<item>The need to tackle languages whose structure is not easily reconciled with the demands of common parser generator, which usually require the grammar to be LALR (1). (The Parse library uses a more tolerant LL scheme, whose only significant constraint is absence of left-recursivity; the library provides mechanisms to detect this condition, which is easy to correct.)</item>
			<item>The need to define several possible semantic treatments on the same syntactic structure.</item>
		</list>
		<paragraph>The last reason may be the most significant practical argument in favor of using Parse. Particularly relevant is the frequent case of a software development environment in which a variety of tools all work on the same basic syntactic structure. For example an environment supporting a programming language such as Pascal or Eiffel may include a compiler, an interpreter, a pretty-printer, software documentation tools (such as Eiffel?s short and flat-short facilities), browsing tools and several other mechanisms that all need to perform semantic actions on software texts that have the same syntactic structure. With common parser generators such as Yacc, the descriptions of syntactic structure and semantic processing are inextricably mixed, so that you normally need one new specification for each tool. This makes design, evolution and reuse of specifications difficult and error-prone.</paragraph>
		<paragraph>In contrast, the Parse library promotes a specification style whereby syntax and semantics are kept separate, and uses inheritance to allow many different semantic descriptions to rely on the same syntactic stem. This will make Parse particularly appropriate in such cases.</paragraph>
		<heading>
			<size>3</size>A word of caution
		</heading>
		<paragraph>At the time of publication the Parse library has not reached the same degree of maturity as the other libraries presented in this book. It should thus be used with some care. You will find at the end of this chapter a few comments about the work needed to bring the library to its full realization.</paragraph>
		<heading>
			<size>2</size>AIMS AND SCOPE OF THE PARSE LIBRARY
		</heading>
		<paragraph>To understand the Parse library it is necessary to appreciate the role of parsing and its place in the more general task of processing documents of various kinds.</paragraph>
		<heading>
			<size>3</size>Basic terminology
		</heading>
		<paragraph>First, some elementary conventions. The word 
			<bold>document</bold> will denote the texts to be parsed. The software systems which perform parsing as part of their processing will be called 
			<bold>document processors</bold>.
		</paragraph>
		<paragraph>Typical document processors are compilers, interpreters, program checkers, specification analyzers and documentation tools. For example the [BENCH] environment contains a number of document processors, used for compiling, documentation and browsing; the language to which they apply is either Eiffel itself or the Lace control language.</paragraph>
		<heading>
			<size>3</size>Parsing, grammars and semantics
		</heading>
		<paragraph>Parsing is seldom an end in itself; rather, it serves as an intermediate step for document processors which perform various other actions.</paragraph>
		<paragraph>Parsing takes care of one of the basic tasks of a document processor: reconstructing the logical organization of a document, which must conform to a certain 
			<bold>syntax</bold> (or structure), defined by a 
			<bold>grammar</bold>.
		</paragraph>
		<note>
			<bold>Note</bold>: the more complete name 
			<bold>syntactic grammar</bold> avoids any confusion with the 
			<italic>lexical</italic> grammars discussed in the 
			<link>
				<url>lexical.html</url>
				<label>previous chapter</label>
			</link>. By default, "grammar" with no further qualification will always denote a syntactic grammar. A syntactic grammar normally relies on a lexical grammar, which gives the form of the most elementary components - the tokens - appearing in the syntactic structure.
		</note>
		<paragraph>Once parsing has reconstructed the structure of a document, the document processor will perform various operations on the basis of that structure. For example a compiler will generate target code corresponding to the original text; a command language interpreter will execute the operations requested in the commands; and a documentation tool such as the short and flat-short commands for Eiffel will produce some information on the parsed document. Such operations are called 
			<bold>semantic actions</bold>. One of the principal requirements on a good parsing mechanism is that it should make it easy to graft semantics onto syntax, by adding semantic actions of many possible kinds to the grammar.
		</paragraph>
		<paragraph>The Parse library provides predefined classes which handle the parsing aspect automatically and provide the hooks for adding semantic actions in a straightforward way. This enables developers to write full document processors - handling both syntax and semantics - simply and efficiently.</paragraph>
		<paragraph>As noted at the beginning of this chapter, it is possible to build a single syntactic base and use it for several processors (such as a compiler and a documentation tool) with semantically different goals, such as compilation and documentation. In the Parse library the semantic hooks take the form of deferred routines, or of routines with default implementations which you may redefine in descendants.</paragraph>
		<heading>
			<size>2</size>LIBRARY CLASSES
		</heading>
		<paragraph>The Parse library contains a small number of classes which cover common document processing applications. The classes, whose inheritance structure was shown at the beginning of this chapter, are:</paragraph>
		<list ordered="false">
			<item>CONSTRUCT, describing the general notion of syntactical construct.</item>
			<item>AGGREGATE, describing constructs of the "aggregate" form.</item>
			<item>CHOICE, describing constructs of the "choice" form.</item>
			<item>REPETITION, describing constructs of the "repetition" form.</item>
			<item>TERMINAL, describing "terminal" constructs with no further structure.</item>
			<item>KEYWORD, describing how to handle keywords.</item>
			<item>L_INTERFACE, providing a simple interface with the lexical analysis process and the Lex library.</item>
			<item>INPUT, describing how to handle the input document.</item>
		</list>
		<heading>
			<size>2</size>EXAMPLES
		</heading> The [BENCH] delivery includes (in the examples/library/parse subdirectory) a simple example using the Parse Library classes. The example is a processor for "documents" which describe computations involving polynomials with variables. The corresponding processor is a system which obtains polynomial specifications and variable values from a user, and computes the corresponding polynomials.
		<paragraph>This example illustrates the most important mechanisms of the Parsing Library and provides a guide for using the facilities described in this chapter. The components of its grammar appear as illustrations in the next sections.</paragraph>
		<heading>
			<size>2</size>CONSTRUCTS AND PRODUCTIONS
		</heading>
		<paragraph>A set of documents possessing common properties, such as the set of all valid Eiffel classes or the set of all valid polynomial descriptions, is called a 
			<bold>language</bold>.
		</paragraph>
		<paragraph>In addition to its lexical aspects, the description of a language includes both syntactic and semantic properties. The grammar - the syntactic specification - describes the structure of the language (for example how an Eiffel class is organized into a number of clauses); the semantic specification defines the meaning of documents written in the language (for example the run-time properties of instances of the class, and the effect of feature calls).</paragraph>
		<paragraph>To discuss the Parse library, it is simpler to consider "language' as a purely syntactic notion; in other words, a language is simply the set of documents conforming to a certain syntactic grammar (taken here to include the supporting lexical grammar). Any semantic aspect will be considered to belong to the province of a specific document processor for the language, although the technique used for specifying the grammar will make it easy to add the specification of the semantics, or several alternative semantic specifications if desired.</paragraph>
		<paragraph>This section explains how you may define the syntactic base - the grammar.</paragraph>
		<heading>
			<size>3</size>Constructs
		</heading>
		<paragraph>A grammar consists of a number of 
			<bold>constructs</bold>, each representing the structure of documents, or document components, called the 
			<bold>specimens</bold> of the construct. For example, a grammar for Eiffel will contain constructs such as Class, Feature_clause and Instruction. A particular class text is a specimen of construct Class.
		</paragraph>
		<paragraph>Each construct will be defined by a 
			<bold>production</bold>, which gives the structure of the construct?s specimens. For example, a production for Class in an Eiffel grammar should express that a class (a specimen of the Class construct) is made of an optional Indexing part, a Class_header, an optional Formal_generics part and so on. The production for Indexing will indicate that any specimen of this construct - any Indexing part - consists of the keyword 
			<bold>indexing</bold> followed by zero or more specimens of Index_clause. 
		</paragraph>
		<paragraph>Although some notations for syntax descriptions such as BNF allow more than one production per construct, the Parse library relies on the convention that every construct is defined by 
			<bold>at most one</bold> production. Depending on whether there is indeed such a production, the construct is either 
			<bold>non-terminal</bold> or 
			<bold>terminal</bold>: 
			<list ordered="false">
				<item>A non-terminal construct (so called because it is defined in terms of others) is specified by a production, which may be of one of three types: aggregate, choice and repetition. The construct will accordingly be called an aggregate, choice or repetition construct.</item>
				<item>A terminal construct has no defining production. This means that it must be defined outside of the syntactical grammar. Terminals indeed come from the 
					<bold>lexical grammar</bold>. Every terminal construct corresponds to a token type (regular expression or keyword) of the lexical grammar, for which the parsing duty will be delegated to lexical mechanisms, assumed in the rest of this chapter to be provided by the Lex library although others may be substituted if appropriate.
				</item>
			</list>
		</paragraph>
		<paragraph>All specimens of terminal constructs are instances of class TERMINAL. A special case is that of keyword constructs, which have a single specimen corresponding to a keyword of the language. For example, 
			<code>
				<keyword>if</keyword>
			</code> is a keyword of Eiffel. Keywords are described by class KEYWORD, an heir of TERMINAL.
		</paragraph>
		<paragraph>The rest of this section concentrates on the parsing-specific part: non-terminal constructs and productions. Terminals will be studied in the discussion of how to interface parsing with lexical analysis.</paragraph>
		<heading>
			<size>3</size>Varieties of non-terminal constructs and productions
		</heading>
		<paragraph>An aggregate production defines a construct whose specimens are obtained by concatenating ("aggregating") specimens of a list of specified constructs, some of which may be optional. For example, the production for construct Conditional in an Eiffel grammar may read:</paragraph>
		<code_block>Conditional [=] if Then_part_list [Else_part] end</code_block>
		<paragraph>This means that a specimen of Conditional (a conditional instruction) is made of the keyword 
			<code>
				<keyword>if</keyword>
			</code>, followed by a specimen of Then_part_list, followed by zero or one specimen of Else_part (the square brackets represent an optional component), followed by the keyword 
			<code>
				<keyword>end</keyword>
			</code>.
		</paragraph>
		<note>
			<bold>Note</bold>: this notation for productions uses conventions similar to those of the book 
			<link>
				<url>http://vig.pearsoned.com/store/product/1,3498,store-3841_isbn-0132479257,00.html</url>
				<label>Eiffel: The Language</label>
			</link>. Keywords are written in 
			<bold>boldface italics</bold> and stand for themselves. Special symbols, such as the semicolon, are written in double quotes, as in ";". The [=] symbol means "is defined as" and is more accurate mathematically than plain =, which, however, is often used for this purpose (see "Introduction to the Theory of Programming Languages", Prentice Hall, 1991, for a more complete discussion of this issue).
		</note>
		<paragraph>A choice production defines a construct whose specimens are specimens of one among a number of specified constructs. For example, the production for construct Type in an Eiffel grammar may read:</paragraph>
		<code_block>Type [=] Class_type | Class_type_expanded | Formal_generic_name | Anchored | Bit_type</code_block>
		<paragraph>This means that a specimen of Type is either a specimen of Class_type, or a specimen of Class_type_expanded etc.</paragraph>
		<paragraph>Finally, a repetition production defines a construct whose specimens are sequences of zero or more specimens of a given construct (called the 
			<bold>base</bold> of the repetition construct), separated from each other by a 
			<bold>separator</bold>. For example, the production for construct Compound in an Eiffel grammar may read
		</paragraph>
		<code_block>Compound [=] {Instruction ";" ...}</code_block>
		<paragraph>This means that a specimen of Compound is made of zero or more specimens of Instruction, each separated from the next (if any) by a semicolon.</paragraph>
		<paragraph>These three mechanisms - aggregate, choice and repetition - suffice to describe the structure of a wide array of practical languages. Properties which cannot be handled by them should be dealt with through 
			<bold>semantic actions</bold>, as explained below.
		</paragraph>
		<heading>
			<size>3</size>An example grammar
		</heading>
		<paragraph>The example directory included in the delivery implements a processor for a grammar describing a simple language for expressing polynomials. A typical "document" in this language is the line</paragraph>
		<paragraph>x; y: x * (y + 8 - (2 * x))</paragraph>
		<paragraph>The beginning of the line, separated from the rest by a colon, is the list of variables used in the polynomial, separated by semicolons. The rest of the line is the expression defining the polynomial.</paragraph>
		<paragraph>Using the conventions defined above, the grammar may be written as:</paragraph>
		<code_block>
				Line    [=] Variables ":" SumVariables [=] {Identifier ";" ...}Sum    [=] {Diff "+" ...}Diff    [=] {Product "-" ...}Product  [=] {Term " * " ...}Term    [=] Simple_var Int_constant NestedNested   [=] "(" Sum ")"
			</code_block>
		<paragraph>This grammar assumes a terminal Identifier, which must be defined as a token type in the lexical grammar. The other terminals are keywords, shown as strings appearing in double quotes, for example "+".</paragraph>
		<heading>
			<size>2</size>PARSING CONCEPTS
		</heading>
		<paragraph>The Parse library supports a parsing mechanism based on the concepts of object-oriented software construction.</paragraph>
		<heading>
			<size>3</size>Class CONSTRUCT
		</heading>
		<paragraph>The deferred class CONSTRUCT describes the general notion of construct; instances of this class and its descendants are specimens of the constructs of a grammar.</paragraph>
		<paragraph>Deferred though it may be, CONSTRUCT defines some useful general patterns; for example, its procedure process appears as:
			<line_break/>
			<code_block>
				</code_block>
			<line_break/> where procedures parse and semantics are expressed in terms of some more specific procedures, which are deferred. This defines a general scheme while leaving the details to descendants of the class.
		</paragraph>
		<paragraph>Such descendants, given in the library, are classes AGGREGATE, CHOICE, REPETITION and TERMINAL. They describe the corresponding types of construct, with features providing the operations for parsing their specimens and applying the associated semantic actions.</paragraph>
		<heading>
			<size>3</size>Building a processor
		</heading>
		<paragraph>To build a processor for a given grammar, you write a class, called a 
			<bold>construct class</bold>, for every construct of the grammar, terminal or non-terminal. The class should inherit from AGGREGATE, CHOICE, REPETITION or TERMINAL depending on the nature of the construct. It describes the production for the construct and any associated semantic actions.
		</paragraph>
		<paragraph>To complete the processor, you must choose a "top construct" for that particular processor, and write a root class. In accordance with the object-oriented method, which implies that "roots" and "tops" should be chosen last, these steps are explained at the end of this chapter.</paragraph>
		<paragraph>The next sections explain how to write construct classes, how to handle semantics, and how to interface parsing with the lexical analysis process. All these tasks rely on a fundamental data abstraction, the notion of 
			<bold>abstract syntax tree</bold>.
		</paragraph>
		<heading>
			<size>3</size>Abstract syntax trees
		</heading>
		<paragraph>The effect of processing a document with a processor built from a combination of construct classes is to build an abstract syntax tree for that document, and to apply any requested semantic actions to that tree.</paragraph>
		<paragraph>The syntax tree is said to be abstract because it only includes important structural information and does not retain the concrete information such as keywords and separators. Such concrete information, sometimes called "syntactic sugar", serves only external purposes but is of no use for semantic processing.</paragraph>
		<paragraph>The combination of Eiffel techniques and libraries yields a very simple approach to building and processing abstract syntax trees. Class CONSTRUCT is a descendant of the Data Structure Library class TWO_WAY_TREE, describing a versatile implementation of trees; so, as a consequence, are CONSTRUCT?s own descendants. The effect of parsing any specimen of a construct is therefore to create an instance of the corresponding construct class. This instance is (among other things) a tree node, and is automatically inserted at its right place in the abstract syntax tree.</paragraph>
		<paragraph>As noted in the discussion of trees, class TWO_WAY_TREE makes no formal distinction between the notions of tree and tree node. So you may identify the abstract syntax tree with the object (instance of CONSTRUCT) representing the topmost construct specimen in the structure of the document being analyzed.</paragraph>
		<heading>
			<size>3</size>The production function
		</heading>
		<paragraph>A construct class describes the syntax of a given construct through a function called production, which is a direct representation of the corresponding production. This function is declared in CONSTRUCT as</paragraph>
		<code_block>
			</code_block>
		<paragraph>Function production remains deferred in classes AGGREGATE, CHOICE and REPETITION. Every effective construct class that you write must provide an effecting of that function. It is important for the efficiency of the parsing process that every effective version of production be a Once function. Several examples of such effectings are given below.</paragraph>
		<paragraph>Classes AGGREGATE, CHOICE, REPETITION and TERMINAL also have a deferred function construct_name of type STRING, useful for tracing and debugging. This function should be effected in every construct class to return the string name of the construct, such as "INSTRUCTION" or "CLASS" for construct classes in a grammar of Eiffel. For efficiency reasons, the construct_name function should also be a Once function. The form of such a function will always be the same, as illustrated by the following example which may appear in the construct class 
			<code>
				<class_name>INSTRUCTION</class_name>
			</code> in a processor for Eiffel:
		</paragraph>
		<code_block>
			</code_block>
		<paragraph>The examples of the next few sections, which explain how to write construct classes, are borrowed from the small "Polynomial" language mentioned above, which may be found in the examples directory in the ISE Eiffel delivery. </paragraph>
		<heading>
			<size>2</size>PREPARING GRAMMARS
		</heading>
		<paragraph>Having studied the Parse library principles, let us see how to write grammar productions for various kinds of construct. The main task is to write the production function for each construct class.</paragraph>
		<paragraph>The production function for a descendant of AGGREGATE will describe how to build a specimen of the corresponding function from a sequence of specimens of each of the constituent constructs. Writing this function from the corresponding production is straightforward.</paragraph>
		<paragraph>As an example, consider the production function of class LINE for the Polynomial example language. The corresponding production is
			<line_break/>
			<code_block>Line [=] Variables ":" Sum</code_block>
			<line_break/> where Variables and Sum are other constructs, and the colon ":" is a terminal. This means that every specimen of Line consists of a specimen of Variables, followed by a colon, followed by a specimen of Sum.
		</paragraph>
		<paragraph>Here is the corresponding production function as it appears in class LINE:</paragraph>
		<code_block>
				</code_block>
		<paragraph>As shown by this example, the production function for an aggregate construct class should declare a local entity (here 
			<code>
				<local_variable>var</local_variable>
			</code> and 
			<code>
				<local_variable>sum</local_variable>
			</code>) for each non-keyword component of the right-hand side, the type of each entity being the corresponding construct class (here VARIABLES and SUM).
		</paragraph>
		<paragraph>The body of the function should begin with
			<code_block>
						</code_block> to create the object containing the result. Then for each non-keyword component, represented by the local entity 
			<code>
				<local_variable>component</local_variable>
			</code> (this applies to 
			<code>
				<local_variable>var</local_variable>
			</code> and 
			<code>
				<local_variable>sum</local_variable>
			</code> in the example), there should be a sequence of two instructions, of the form
		</paragraph>
		<code_block>
					
				</code_block>
		<paragraph>For any keyword of associated string 
			<italic>symbol</italic>, such as the colon ":" in the example, there should be a call to 
		</paragraph>
		<code_block>
					</code_block>
		<paragraph>The order of the various calls to put (for non-keywords) and keyword (for keywords) must be the order of the components in the production. Also, every 
			<code>
				<keyword>create</keyword>
			</code>
			<code>
				<local_variable>component</local_variable>
			</code>
			<code>
				<symbol>.</symbol>
			</code>make instruction must occur before the corresponding call to put 
			<code>
				<symbol>(</symbol>
			</code>
			<code>
				<local_variable>symbol</local_variable>
			</code>
			<code>
				<symbol>)</symbol>
			</code>.
		</paragraph>
		<paragraph>All components in the above example are required. In the general case an aggregate production may have optional components. To signal that a component component of the right-hand side is optional, include a call of the form</paragraph>
		<code_block>
						
			<local_variable>component</local_variable>
						
			<symbol>.</symbol>set_optional
		</code_block>
		<paragraph>This call may appear anywhere after the corresponding 
			<code>
				<keyword>create</keyword>
			</code>
			<code>
				<local_variable>component</local_variable>
			</code> instruction. The recommended place is just after the call to put, as in
		</paragraph>
		<code_block>
						</code_block>
		<heading>
			<size>3</size>Choices
		</heading>
		<paragraph>The production function for a descendant of CHOICE will describe how to build a specimen of the corresponding function as a specimen of one of the alternative constructs.</paragraph>
		<paragraph>As an example, consider the production function of class TERM for the Polynomial example language. The corresponding production is</paragraph>
		<code_block>Term [=] Simple_var Poly_integer Nested</code_block>
		<line_break/> where Simple_var, Poly_integer and Nested are other constructs. This means that every specimen of Term consists of one specimen of any one of these three constructs. Here is the corresponding production function as it appears in class TERM: 
		<code_block>
						</code_block>
		<paragraph>As shown by this example, the production function for a choice construct class must declare a local entity - here 
			<code>
				<local_variable>id</local_variable>
			</code>, 
			<code>
				<local_variable>val</local_variable>
			</code> and 
			<code>
				<local_variable>nest</local_variable>
			</code> - for each alternative component of the right-hand side. The type of each entity is the corresponding construct class - here SIMPLE_VAR, POLY_INTEGER and NESTED.
		</paragraph>
		<paragraph>The body of the function must begin by</paragraph>
		<code_block>
							</code_block>
		<paragraph>Then for each alternative component represented by a local entity component (in the example this applies to 
			<code>
				<local_variable>id</local_variable>
			</code>, 
			<code>
				<local_variable>val</local_variable>
			</code> and 
			<code>
				<local_variable>nest</local_variable>
			</code>) there should be two instructions of the form
		</paragraph>
		<code_block>
							
						</code_block>
		<note>
			<bold>Caution</bold>: the order of the various calls to put is irrelevant in principle. When a document is parsed, however, the choices will be tried in the order given; so if you know that certain choices occur more frequently than others it is preferable to list them first to speed up the parsing process.
		</note>
		<heading>
			<size>3</size>Repetitions
		</heading>
		<paragraph>The production function for a descendant of REPETITION will describe how to build a specimen of the corresponding function as a sequence or zero or more (or, depending on the grammar, one or more) specimens of the base construct. The class must also effect a feature separator of type STRING, usually as a constant attribute. (This feature is introduced as deferred in class REPETITION.)</paragraph>
		<paragraph>As an example, consider the construct Variables in the Polynomial example language. The right-hand side of the corresponding production is
			<line_break/>
			<code_block>Variables [=] {Identifier ";" ...}</code_block>
			<line_break/> where Identifier is another construct, and the semicolon ";" is a terminal. This means that every specimen of Variables consists of zero or more specimens of Identifier, separated from each other (if more than one) by semicolons.
		</paragraph>
		<paragraph>Here are the corresponding production function and separator attribute as they appear in class VARIABLES:</paragraph>
		<code_block>
							
						</code_block>
		<paragraph>As shown by this example, function production is built along the same ideas as for aggregates and choices, except that here only one component, 
			<code>
				<local_variable>base</local_variable>
			</code>, is required; its type must be the class corresponding to the construct serving as the base of the repetition, VAR in the example.
		</paragraph>
		<heading>
			<size>2</size>INTERFACE TO LEXICAL ANALYSIS
		</heading>
		<paragraph>One more type of construct class remains to be discussed: terminal construct classes. Since terminal constructs serve to elevate lexical tokens (regular expressions and keywords) to the dignity of syntactical construct, we must first take a look at how the Parse library classes collaborate with their counterparts in the Lex library.</paragraph>
		<heading>
			<size>3</size>The notion of lexical interface class
		</heading>
		<paragraph>To parse a document, you need to get tokens from a lexical analyzer. This is achieved by making some construct classes, in particular those describing terminals, descendants of one of the lexical classes.</paragraph>
		<paragraph>The best technique is usually to write a class covering the lexical needs of the language at hand, from which all construct classes that have some lexical business will inherit. Such a class is called a lexical interface class.</paragraph>
		<paragraph>Lexical interface classes usually follow a common pattern. To take advantage of this uniformity, the Parse library includes a deferred class L_INTERFACE which describes that pattern. Specific lexical interface classes may be written as descendants of L_INTERFACE.</paragraph>
		<paragraph>L_INTERFACE is a simple deferred class, with a deferred procedure obtain_analyzer. It is an heir of METALEX. 
			<heading>
				<size>3</size>Obtaining a lexical analyzer
			</heading>
			<paragraph>An effective descendant of L_INTERFACE must define procedure obtain_analyzer so that it records into the lexical analyzer the regular expressions and keywords of the language at hand. In writing obtain_analyzer you may use any one of three different techniques, each of which may be the most convenient depending on the precise context, to obtain the required lexical analyzer: 
				<list ordered="false">
					<item>You may build the lexical analyzer by defining its regular expressions one by one, using the procedures described in the presentation of METALEX, in particular put_expression and put_keyword.</item>
					<item>You may use use procedure retrieve_analyzer from METALEX to retrieve an analyzer which a previous session saved into a file.</item>
					<item>Finally, you may write a lexical grammar file (or reuse an existing one) and process it on the spot by using procedure read_grammar from METALEX.</item>
				</list>
			</paragraph>
			<heading>
				<size>3</size>A lexical interface class
			</heading>
			<paragraph>An example of a lexical interface class is POLY_LEX for the Polynomial example language. Here is the complete text of that class:</paragraph>
			<code_block>
								
							</code_block>
			<paragraph>This class illustrates the straightforward scheme for writing lexical interface classes. It introduces constants such as Special to represent the regular expressions supported, and effects procedure obtain_analyzer. The role of this procedure is to define lexical conventions (here through calls to ignore_case and keywords_ignore_case), to record the regular expressions (through calls to put_expression, packaged in a procedure build_expressions for clarity), and records the keywords (through calls to put_keyword, packaged in build_keywords).</paragraph>
			<paragraph>All the classes of a document processor that need to interact with the lexical analysis should inherit from a lexical interface class such as POLY_LEX. This is true in particular of the root class of a processor, as discussed below.</paragraph>
			<heading>
				<size>3</size>More on terminal constructs
			</heading>
			<paragraph>Terminal construct classes are examples of classes that need to interact with the lexical analysis, and should thus inherit from the lexical interface class.</paragraph>
			<paragraph>Class TERMINAL includes a deferred function token_type of type INTEGER. Every effective descendant of TERMINAL should effect this feature as a constant attribute, whose value is the code for the associated regular expression, obtained from the lexical interface class. As every other construct class, such a descendant should also effect construct_name as a Once function. For example, in the Polynomial language, class INT_CONSTANT has the following text: 
				<code_block>
									
								</code_block>
				<heading>
					<size>2</size>SPECIFYING THE SEMANTICS
				</heading>
				<paragraph>As mentioned at the beginning of this chapter, parsing is usually done not for itself but as a way to perform some semantic processing. The Parsing Library classes define the general framework for grafting such semantics onto a syntactical stem.</paragraph>
				<heading>
					<size>3</size>Semantic procedures
				</heading>
				<paragraph>The principal procedures for defining semantic actions are pre_action and post_action. These are features of class CONSTRUCT. Procedure pre_action describes the actions to be performed before a construct has been recognized; post_action, the actions to be performed after a construct has been recognized.</paragraph>
				<paragraph>As defined in CONSTRUCT, both and post_action do nothing by default. Any construct class which is a descendant of CONSTRUCT may redefine one or both so that they will perform the semantic actions that the document processor must apply to specimens of the corresponding construct. These procedures are called automatically during processing, before and after the corresponding structures have been parsed.</paragraph>
				<paragraph>For TERMINAL, only one semantic action makes sense. To avoid any confusion, post_action is renamed action in that class and pre_action is renamed unused_pre_action to indicate that it is irrelevant.</paragraph>
				<paragraph>Often, the semantic procedures need to compute various elements of information. These may be recorded using appropriate attributes of the corresponding construct classes.</paragraph>
				<note>
					<bold>Note</bold>: readers familiar with the theory of parsing and compiling will see that this scheme, using the attributes of Eiffel classes, provides a direct implementation of the "attribute grammar" mechanism.
				</note>
				<heading>
					<size>3</size>Polynomial semantics
				</heading>
				<paragraph>As an example let us examine the semantics of the Product construct for the polynomial language. It is a repetition construct, with Term as the base construct; in other words a specimen of Product is a sequence of one or more terms, representing the product term
					<sub_script>1</sub_script> * term
					<sub_script>2</sub_script> ... * term
					<sub_script>n</sub_script>. Here is the post_action procedure in the corresponding class PRODUCT:
				</paragraph>
				<code_block>
								</code_block>
				<paragraph>Here each relevant construct class has an attribute info used to record the semantic information associated with polynomials and their components, such as child_value, an INTEGER. The post_action takes care of computing the product of all child_values for the children. First, of course, post_action must recursively be applied to each child, to compute its own child_value.</paragraph>
				<note>
					<bold>Note</bold>: recall that an instance of CONSTRUCT is also a node of the abstract syntax tree, so that all the TWO_WAY_TREE features such as child_value, child_start, child_after and many others are automatically available to access the syntactical structure.
				</note>
				<heading>
					<size>3</size>Keeping syntax and semantics separate
				</heading>
				<paragraph>For simple examples such as the Polynomial language, it is convenient to use a single class to describe both the syntax of a construct (through the production function and associated features) and its semantics (action routines and associated features).</paragraph>
				<paragraph>For more ambitious languages and processors, however, it is often preferable to keep the two aspects separate. Such separation of syntax and semantics, and in particular the sharing of the same syntax for different processors with different semantic actions, is hard or impossible to obtain with traditional document processing tools such as Yacc on Unix. Here is how to achieve it with the Parse library: 
					<list ordered="false">
						<item>First write purely 
							<bold>syntactic classes</bold>, that is to say construct classes which only effect the syntactical part (in particular function production). As a consequence, these classes usually remain deferred. The recommended convention for such syntactic classes is to use names beginning with 
							<code>
								<class_name>S_</class_name>
							</code>, for example 
							<code>
								<class_name>S_INSTRUCTION</class_name>
							</code> or 
							<code>
								<class_name>S_LOOP</class_name>
							</code>.
						</item>
						<item>Then for each construct for which a processor defines a certain semantics, define another class, called a 
							<bold>semantic class</bold>, which inherits from the corresponding syntactic class. The recommended convention for semantic classes is to give them names which directly reflect the corresponding construct name, as in 
							<code>
								<class_name>INSTRUCTION</class_name>
							</code> or 
							<code>
								<class_name>LOOP</class_name>
							</code>.
						</item>
					</list>
				</paragraph>
				<paragraph>To build a semantic class in in step 2 it is often convenient to use multiple inheritance from a syntactic class and a "semantics-only" class. For example in a processor for Eiffel class 
					<code>
						<class_name>INSTRUCTION</class_name>
					</code> may inherit from both 
					<code>
						<class_name>IS_INSTRUCTION</class_name>
					</code> and from a semantics-only class 
					<code>
						<class_name>INSTRUCTION_PROPERTIES</class_name>
					</code> which introduces the required semantic features.
				</paragraph>
				<paragraph>One of the advantages of this scheme is that it makes it easy to associate two or more types of processing with a single construct, by keeping the same syntactic class (such as 
					<code>
						<class_name>IS_INSTRUCTION</class_name>
					</code>) but choosing a different pure-semantics class each time.
				</paragraph>
				<paragraph>As noted earlier in this chapter, this is particularly useful in an environment where different processors need to perform differents actions on specimens of the same construct. In an Eiffel environment, for example, processors that manipulate classes and other Eiffel construct specimens may include a compiler, an interpreter, a flattener (producing the flat form), a class abstracter (producing the short or flat-short form), and various browsing tools such as those of ISE Eiffel.</paragraph>
				<paragraph>For obvious reasons of convenience and ease of maintenance, it is desirable to let these processors share the same syntactic descriptions. The method just described, relying on multiple inheritance, achieves this goal.</paragraph>
				<heading>
					<size>2</size>HOW PARSING WORKS
				</heading>
				<paragraph>Classes AGGREGATE, CHOICE, TERMINAL and REPETITION are written in such a way that you do not need to take care of the parsing process. They make it possible to parse any language built according to the rules given - with one limitation, left recursion, discussed below. You can then concentrate on writing the interesting part - semantic processing.</paragraph>
				<paragraph>To derive the maximum benefit from the Parse library, however, it is useful to gain a little more insight into the way parsing works. Let us raise the veil just enough to see any remaining property that is relevant to the building of parsers and document processors.</paragraph>
				<heading>
					<size>3</size>The parsing technique
				</heading>
				<paragraph>The Parse library relies on a general approach known as 
					<bold>recursive descent</bold>, meaning that various choices will be tried in sequence and recursively to recognize a certain specimen.
				</paragraph>
				<paragraph>If a choice is attempted and fails (because it encounters input that does not conform to what is expected), the algorithm will try remaining choices, after having moved the input cursor back to where it was before the choice that failed. This process is called 
					<bold>backtracking</bold>. It is handled by the parsing algorithms in an entirely automatic fashion, without programmer intervention.
				</paragraph>
				<heading>
					<size>3</size>Left recursion
				</heading>
				<paragraph>Recursive descent implies the danger of infinite looping when parsing is attempted for left-recursive productions of the form
					<line_break/>
					<code_block>A [=] A ...</code_block>
					<line_break/> or, more generally, cases in which the left recursion is indirect, as in
					<line_break/>
					<code_block> A [=] B ...
						<line_break/> B [=] C ...
						<line_break/> ...
						<line_break/> L [=] A ...
						<line_break/>
									
					</code_block>
				</paragraph>
				<paragraph>Direct left recursion is easy to avoid, but indirect recursion may sneak in in less trivial ways.</paragraph>
				<paragraph>To determine whether the production for a construct is directly or indirectly left-recursive, use the query left_recursion from class CONSTRUCT.</paragraph>
				<heading>
					<size>3</size>Backtracking and the commit procedure
				</heading>
				<paragraph>Another potential problem may arise from too much backtracking. In contrast with left recursion, this is a performance issue, not a threat to the correctness of the parsing algorithm. Automatic backtracking is in fact essential to the generality and flexibility of the recursive descent parsing algorithm; but too much of it may degrade the efficiency of the parsing mechanism.</paragraph>
				<paragraph>Two techniques are available to minimize backtracking. One, mentioned above, is to organize the production functions for choice construct classes so that they list the most frequent cases first. The other is to use the commit procedure in the production functions for aggregate constructs.</paragraph>
				<paragraph>A call to commit in an aggregate A is a hint to the parser, which means:</paragraph>
				<code_block>
				</code_block>
				<paragraph>Such a hint is useful when you want to let the parser benefit from some higher-level knowledge about the grammar, which is not directly deducible from the way the productions have been written.</paragraph>
				<paragraph>Here is an example. The production function for NESTED in the Polynomial language, which attempts to parse specimens of the form
					<line_break/>
					<code_block>(s)</code_block>
					<line_break/> where 
					<italic>s</italic> is a specimen of SUM, is written as
				</paragraph>
				<code_block>
								</code_block>
				<paragraph>The commit after the recognition of the keyword "(" is there to use the following piece of higher-level knowledge:</paragraph>
				<code_block>No choice production of the grammar that has NESTED as one of its alternatives has another alternative construct whose specimens could begin with an opening parenthesis "(".</code_block>
				<paragraph>Because of this property, if the parser goes so far as to recognize an opening parenthesis as part of parsing any construct 
					<code>
						<class_name>C</class_name>
					</code> for which NESTED is an alternative, but further tokens do not match the structure of NESTED specimens, then we will have failed to recognize not only a NESTED but also a 
					<code>
						<class_name>C</class_name>
					</code>.
				</paragraph>
				<note>
					<bold>Note</bold>: some readers will have recognized commit as being close to the Prolog "cut" mechanism.
				</note>
				<paragraph>In this example, NESTED is used in only one right-hand side production: the choice production for TERM, for which the other alternatives are SIMPLE_VAR and POLY_INTEGER, none of whose specimens can include an opening parenthesis.</paragraph>
				<paragraph>The use of commit assumes global knowledge about the grammar and its future extensions, which is somewhat at odds with the evolutionary approach suggested by the Eiffel method. Applied improperly, this mechanism could lead to the rejection of valid texts as invalid. Used with care, however, it helps in obtaining high-performance parsing without impairing too much the simplicity of preparing parsers and other document processors.</paragraph>
				<heading>
					<size>2</size>BUILDING A DOCUMENT PROCESSOR
				</heading>
				<paragraph>We are ready now to put together the various elements required to build a document processor based on the Parse library.</paragraph>
				<heading>
					<size>3</size>The overall picture
				</heading>
				<paragraph>The documents to be processed will be specimens of a certain construct. This construct is called the 
					<bold>top construct</bold> for that particular processing.
				</paragraph>
				<warning>
					<bold>Caution</bold>: be sure to note that with the Parse library there is no room for a concept of top construct of a 
					<bold>grammar</bold>: the top construct is only defined with respect to a particular processor for that grammar.
					<line_break/> Attempting to define the top of a grammar would be contrary to the object-oriented approach, which de-emphasizes any notion of top component of a system.
					<line_break/> Different processors for the same grammar may use different top constructs.
				</warning>
				<paragraph>A document processor will be a particular system made of construct classes, complemented by semantic classes, and usually by other auxiliary classes. One of the construct classes corresponds to the top construct and is called the 
					<bold>top construct class</bold>.
				</paragraph>
				<note>
					<bold>Note</bold>: this notion of top construct class has a natural connection to the notion of root class of a system, as needed to get executable software. The top construct class could indeed be used as root of the processor system. In line with the previous discussion, however, it appears preferable to keep the top construct class (which only depends on the syntax and remains independent of any particular processor) separate from the system?s root class. With this approach the root class will often be a descendant of the top construct class.
					<line_break/> This policy was adopted for the Polynomial language example as it appears in the delivery: the processor defined for this example uses LINE as the top construct class; the root of the processor system is a class PROCESS, which inherits from LINE.
				</note>
				<heading>
					<size>3</size>Steps in the execution of a document processor
				</heading>
				<paragraph>As any root class of a system, the root of a document processor must have a creation procedure which starts the execution. Here the task of this class is the following: 
					<list ordered="true">
						<item>Define an object representing the input document to be processed; this will be an instance of class INPUT.</item>
						<item>Obtain a lexical analyzer applicable to the language, and connect it with the document.</item>
						<item>Select an input file, containing the actual document to be processed.</item>
						<item>Process the document: in other words, parse it and, if parsing is successful, apply the semantics.</item>
						<item>To execute these steps in a simple and convenient fashion, it is useful to declare the root class as a descendant of the lexical interface class. The root class, being an heir to the top construct class, will also be a descendant of CONSTRUCT.</item>
					</list>
				</paragraph>
				<heading>
					<size>3</size>Connecting with lexical analysis
				</heading>
				<paragraph>To achieve the effect of steps 
					<link>
						<url>#step_e1</url>
						<label>1</label>
					</link> and 
					<link>
						<url>#step_e2</url>
						<label>2</label>
					</link>, a simple call instruction suffices: just call the procedure build, inherited from L_INTERFACE, using as argument document, a feature of type INPUT, obtained from METALEX (the lexical analyzer generator class) through L_INTERFACE. The call, then, is just:
				</paragraph>
				<code_block>build
					<symbol>(</symbol>document
					<symbol>)</symbol>
								
				</code_block>
				<paragraph>Although you may use this line as a recipe with no need for further justification, it is interesting to see what build does. Feature document describes the input document to be processed; it is introduced as a Once function in class CONSTRUCT to ensure that all instances of CONSTRUCT share a single document - in other words, that all processing actions apply to the same document. The text of build is:</paragraph>
				<code_block>
								</code_block>
				<paragraph>The call to obtain_analyzer defines the regular grammar for the language at hand. Recall that obtain_analyzer is deferred in L_INTERFACE; its definition for the POLY_LEX example was given above. The call to make_analyzer freezes the regular grammar and produces a usable lexical analyzer, available through the attribute analyzer obtained from METALEX. Finally, the call to set_lexical, a procedure of class INPUT, ensures that all lexical analysis operations will use analyzer as the lexical analyzer.</paragraph>
				<heading>
					<size>3</size>Starting the actual processing
				</heading>
				<paragraph>The call build 
					<code>
						<symbol>(</symbol>
					</code>document takes care of steps 
					<link>
						<url>#step_e1</url>
						<label>1</label>
					</link> and 
					<link>
						<url>#step_e2</url>
						<label>2</label>
					</link> of the root?s creation procedure. Step 
					<link>
						<url>#step_e3</url>
						<label>3</label>
					</link> selects the file containing the input document; this is achieved through the call
					<line_break/>
					<code_block>document
						<symbol>.</symbol>set_input_file
						<symbol>(</symbol>
										
						<local_variable>some_file_name</local_variable>
										
						<symbol>)</symbol>
									
					</code_block>
					<line_break/> where set_input_file, from class INPUT, has a self-explanatory effect.
				</paragraph>
				<paragraph>Finally, step 
					<link>
						<url>#step_e4</url>
						<label>4</label>
					</link> (processing the document) is simply a call to procedure process, obtained from CONSTRUCT. Recall that this procedure simply executes
					<line_break/>
					<code_block>
									</code_block>
					<line_break/>
					<heading>
						<size>3</size>The structure of a full example
					</heading>
					<paragraph>The polynomial example provides a simple example of a full document processor, which you may use as a guide for your own processors. The root class of that example is PROCESS. Its creation procedure, make, follows the above scheme precisely; here is its general form: 
						<code_block>
										</code_block>
						<paragraph>Although it covers a small language, this example may serve as a blueprint for most applications of the Parse library.</paragraph>
						<heading>
							<size>2</size>FUTURE WORK
						</heading>
						<paragraph>It was mentioned at the beginning of this chapter that further work is desirable to make the Parse library reach its full bloom. Here is a glimpse of future improvements.</paragraph>
						<heading>
							<size>3</size>Expressions
						</heading>
						<paragraph>Many languages include an expression construct having the properties of traditional arithmetic expressions: 
							<list ordered="false">
								<item>An expression is a succession of basic operands and operators.</item>
								<item>The basic operands are lexical elements, such as identifiers and constants.</item>
								<item>Operators are used in prefix mode (as in 
									<italic>- a</italic>) or infix mode (as in 
									<italic>b - a</italic>).
								</item>
								<item>Each operator has a precedence level; precedence levels determine the abstract syntactic structure of expressions and consequently their semantics. For example the abstract structure of 
									<italic>a + b * c</italic> shows this expression to be the application of the operator 
									<italic>+</italic> to 
									<italic>a</italic> and to the application of the operator 
									<italic>*</italic> to 
									<italic>b</italic> and 
									<italic>c</italic>. That this is the correct interpretation of the instruction follows from the property that 
									<italic>*</italic> has a higher precedence ("binds more tightly") than 
									<italic>+</italic>.
								</item>
								<item>Parentheses pairs, such as ( ) or [ ], can be used to enforce a structure different from what the precedence rules would imply, as in 
									<italic>(a + b) * c</italic>.
								</item>
								<item>Some infix operators may be applied to more than two arguments; in this case it must be clear whether they are right-associative (in other words, 
									<italic>a ^ b ^ c</italic> means 
									<italic>a ^ (b ^ c)</italic>, the conventional interpretation if ^ denotes the power operator) or left-associative.
								</item>
							</list>
						</paragraph>
						<paragraph>It is of course possible to apply the Parse library in its current state to support expressions, as illustrated by this extract from the Polynomial grammar given in full above:</paragraph>
						<code_block>	
						</code_block>
						<paragraph>The problem then is not expressiveness but efficiency. For such expressions the recursive descent technique, however well adapted to the higher-level structures of a language, takes too much time and generates too many tree nodes. Efficient bottom-up parsing techniques are available for this case.</paragraph>
						<paragraph>The solution is straightforward: write a new heir 
							<code>
								<class_name>EXPRESSION</class_name>
							</code> to class CONSTRUCT. The preceding discussion of expressions and their properties suggests what kinds of feature this class will offer: define a certain terminal as operator, define a terminal as operand type, set the precedence of an operator, set an operator as left-associative or right-associative and so on. Writing this class based on this discussion is indeed a relatively straightforward task, which can be used as a programming exercise.
						</paragraph>
						<paragraph>Beyond the addition of an 
							<code>
								<class_name>EXPRESSION</class_name>
							</code> class, some changes in the data structures used by Parse may also help improve the efficiency of the parsing process.
						</paragraph>
						<heading>
							<size>3</size>Yooc
						</heading>
						<paragraph>To describe the syntax of a language, it is convenient to use a textual format such as the one that has served in this chapter to illustrate the various forms of production. The correspondence between such a format and the construct classes is straightforward; for example, as explained above, the production
							<line_break/>
							<code_block>Line [=] Variables ":" Sum</code_block>
							<line_break/> will yield the class 
							<code_block>
							</code_block>
							<paragraph>This transformation of the textual description of the grammar into its equivalent Eiffel form is simple and unambiguous; but it is somewhat annoying to have to perform it manually.</paragraph>
							<paragraph>A tool complementing the Parse library and known as YOOC ("Yes! an Object-Oriented Compiler", a name meant as an homage to the venerable Yacc) has been planned for future releases of Parse. Yooc, a translator, will take a grammar specification as input and transform it into a set of parsing classes, all descendants of CONSTRUCT and built according to the rules defined above. The input format for syntax specification, similar to the conventions used throughout this chapter, is a variant of LDL (Language Description Language), a component of the ArchiText structural document processing system.</paragraph>
							<heading>
								<size>3</size>Further reading
							</heading>
							<paragraph>The following article describes some advanced uses of the Parse library as well as a Yooc-like translator called PG: Per Grape and Kim Walden: Automating the Development of Syntax Tree Generators for an Evolving Language, in Proceedings of TOOLS 8 (Technology of Object-Oriented Languages and Systems), Prentice Hall, 1992, pages 185-195.</paragraph>
						</paragraph>
					</paragraph>
				</paragraph>
			</paragraph>
		</paragraph>
	</paragraph></document>